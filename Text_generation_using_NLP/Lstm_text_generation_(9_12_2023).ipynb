{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mu86zkAYZRlx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from tensorflow import set_random_seed\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "import keras.utils as ku\n",
        "\n",
        "from numpy.random import seed\n",
        "# set_random_seed(2)\n",
        "seed(1)\n",
        "\n",
        "import string, os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "A0zoGRtSiJtA"
      },
      "outputs": [],
      "source": [
        "file_path = '001ssb.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bScdjGQ0iZBQ",
        "outputId": "719d0657-7ac5-4dd9-bca0-a76e2ba0bb38"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12626"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "file_ = open(file_path, 'r')\n",
        "data = file_.read()\n",
        "sentences = data.split('.')\n",
        "length = int(len(sentences)/2-1)\n",
        "sentences = sentences[:length]\n",
        "length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49S22TpGjkv1",
        "outputId": "1b490b48-6b49-4676-b585-93750145ec49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['a game of thrones book one of a song of ice and fire by george r', ' r', ' martin prologue we should start back gared urged as the woods began to grow dark around them', ' the wildlings are dead', ' do the dead frighten you ser waymar royce asked with just the hint of a smile', ' gared did not rise to the bait', ' he was an old man past fifty and he had seen the lordlings come and go', ' dead is dead he said', ' we have no business with the dead', ' are they dead royce asked softly']\n",
            "12626\n"
          ]
        }
      ],
      "source": [
        "def clean_text(txt):\n",
        "  txt = ''.join(v for v in txt if v not in string.punctuation).lower()\n",
        "  txt = txt.encode(\"utf-8\").decode(\"ascii\", 'ignore')\n",
        "  txt = txt.replace(\"\\n\", \"\")\n",
        "  return txt\n",
        "\n",
        "corpus = [clean_text(x) for x in sentences]\n",
        "print(corpus[:10])\n",
        "print(len(corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VB3RUCSdkGM6"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "\n",
        "def get_sequence_of_tokens(corpus):\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  total_words = len(tokenizer.word_index) + 1\n",
        "  input_sequences = []\n",
        "  for line in corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\n",
        "    for i in range(1, len(token_list)):\n",
        "      n_gram_sequence = token_list[:i+1]\n",
        "      input_sequences.append(n_gram_sequence)\n",
        "  return input_sequences, total_words\n",
        "\n",
        "inp_sequences, total_words = get_sequence_of_tokens(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hm-moTNlm527",
        "outputId": "3a482cb1-d4c1-4924-ff7d-aeca662c3a84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[3, 990],\n",
              " [3, 990, 5],\n",
              " [3, 990, 5, 1574],\n",
              " [3, 990, 5, 1574, 914],\n",
              " [3, 990, 5, 1574, 914, 49],\n",
              " [3, 990, 5, 1574, 914, 49, 5],\n",
              " [3, 990, 5, 1574, 914, 49, 5, 3],\n",
              " [3, 990, 5, 1574, 914, 49, 5, 3, 1214],\n",
              " [3, 990, 5, 1574, 914, 49, 5, 3, 1214, 5],\n",
              " [3, 990, 5, 1574, 914, 49, 5, 3, 1214, 5, 456],\n",
              " [3, 990, 5, 1574, 914, 49, 5, 3, 1214, 5, 456, 2],\n",
              " [3, 990, 5, 1574, 914, 49, 5, 3, 1214, 5, 456, 2, 299],\n",
              " [3, 990, 5, 1574, 914, 49, 5, 3, 1214, 5, 456, 2, 299, 64],\n",
              " [3, 990, 5, 1574, 914, 49, 5, 3, 1214, 5, 456, 2, 299, 64, 2094],\n",
              " [3, 990, 5, 1574, 914, 49, 5, 3, 1214, 5, 456, 2, 299, 64, 2094, 885],\n",
              " [1575, 5371],\n",
              " [1575, 5371, 75],\n",
              " [1575, 5371, 75, 190],\n",
              " [1575, 5371, 75, 190, 945],\n",
              " [1575, 5371, 75, 190, 945, 60],\n",
              " [1575, 5371, 75, 190, 945, 60, 601],\n",
              " [1575, 5371, 75, 190, 945, 60, 601, 991],\n",
              " [1575, 5371, 75, 190, 945, 60, 601, 991, 17],\n",
              " [1575, 5371, 75, 190, 945, 60, 601, 991, 17, 1],\n",
              " [1575, 5371, 75, 190, 945, 60, 601, 991, 17, 1, 728],\n",
              " [1575, 5371, 75, 190, 945, 60, 601, 991, 17, 1, 728, 230],\n",
              " [1575, 5371, 75, 190, 945, 60, 601, 991, 17, 1, 728, 230, 4],\n",
              " [1575, 5371, 75, 190, 945, 60, 601, 991, 17, 1, 728, 230, 4, 992],\n",
              " [1575, 5371, 75, 190, 945, 60, 601, 991, 17, 1, 728, 230, 4, 992, 213],\n",
              " [1575, 5371, 75, 190, 945, 60, 601, 991, 17, 1, 728, 230, 4, 992, 213, 115],\n",
              " [1575,\n",
              "  5371,\n",
              "  75,\n",
              "  190,\n",
              "  945,\n",
              "  60,\n",
              "  601,\n",
              "  991,\n",
              "  17,\n",
              "  1,\n",
              "  728,\n",
              "  230,\n",
              "  4,\n",
              "  992,\n",
              "  213,\n",
              "  115,\n",
              "  34],\n",
              " [1, 2343],\n",
              " [1, 2343, 55],\n",
              " [1, 2343, 55, 201],\n",
              " [67, 1],\n",
              " [67, 1, 201],\n",
              " [67, 1, 201, 1372],\n",
              " [67, 1, 201, 1372, 11],\n",
              " [67, 1, 201, 1372, 11, 52],\n",
              " [67, 1, 201, 1372, 11, 52, 759],\n",
              " [67, 1, 201, 1372, 11, 52, 759, 553],\n",
              " [67, 1, 201, 1372, 11, 52, 759, 553, 121],\n",
              " [67, 1, 201, 1372, 11, 52, 759, 553, 121, 19],\n",
              " [67, 1, 201, 1372, 11, 52, 759, 553, 121, 19, 193],\n",
              " [67, 1, 201, 1372, 11, 52, 759, 553, 121, 19, 193, 1],\n",
              " [67, 1, 201, 1372, 11, 52, 759, 553, 121, 19, 193, 1, 1726],\n",
              " [67, 1, 201, 1372, 11, 52, 759, 553, 121, 19, 193, 1, 1726, 5],\n",
              " [67, 1, 201, 1372, 11, 52, 759, 553, 121, 19, 193, 1, 1726, 5, 3],\n",
              " [67, 1, 201, 1372, 11, 52, 759, 553, 121, 19, 193, 1, 1726, 5, 3, 256],\n",
              " [601, 62],\n",
              " [601, 62, 21],\n",
              " [601, 62, 21, 993],\n",
              " [601, 62, 21, 993, 4],\n",
              " [601, 62, 21, 993, 4, 1],\n",
              " [601, 62, 21, 993, 4, 1, 5372],\n",
              " [6, 8],\n",
              " [6, 8, 76],\n",
              " [6, 8, 76, 111],\n",
              " [6, 8, 76, 111, 50],\n",
              " [6, 8, 76, 111, 50, 290],\n",
              " [6, 8, 76, 111, 50, 290, 1373],\n",
              " [6, 8, 76, 111, 50, 290, 1373, 2],\n",
              " [6, 8, 76, 111, 50, 290, 1373, 2, 6],\n",
              " [6, 8, 76, 111, 50, 290, 1373, 2, 6, 13],\n",
              " [6, 8, 76, 111, 50, 290, 1373, 2, 6, 13, 222],\n",
              " [6, 8, 76, 111, 50, 290, 1373, 2, 6, 13, 222, 1],\n",
              " [6, 8, 76, 111, 50, 290, 1373, 2, 6, 13, 222, 1, 3241],\n",
              " [6, 8, 76, 111, 50, 290, 1373, 2, 6, 13, 222, 1, 3241, 114],\n",
              " [6, 8, 76, 111, 50, 290, 1373, 2, 6, 13, 222, 1, 3241, 114, 2],\n",
              " [6, 8, 76, 111, 50, 290, 1373, 2, 6, 13, 222, 1, 3241, 114, 2, 127],\n",
              " [201, 26],\n",
              " [201, 26, 201],\n",
              " [201, 26, 201, 6],\n",
              " [201, 26, 201, 6, 20],\n",
              " [75, 33],\n",
              " [75, 33, 28],\n",
              " [75, 33, 28, 847],\n",
              " [75, 33, 28, 847, 19],\n",
              " [75, 33, 28, 847, 19, 1],\n",
              " [75, 33, 28, 847, 19, 1, 201],\n",
              " [55, 27],\n",
              " [55, 27, 201],\n",
              " [55, 27, 201, 553],\n",
              " [55, 27, 201, 553, 121],\n",
              " [55, 27, 201, 553, 121, 789],\n",
              " [53, 1215],\n",
              " [53, 1215, 33],\n",
              " [53, 1215, 33, 75],\n",
              " [53, 1215, 33, 75, 40],\n",
              " [53, 1215, 33, 75, 40, 137],\n",
              " [53, 1215, 33, 75, 40, 137, 34],\n",
              " [53, 1215, 33, 75, 40, 137, 34, 601],\n",
              " [53, 1215, 33, 75, 40, 137, 34, 601, 20],\n",
              " [43, 6],\n",
              " [43, 6, 512],\n",
              " [43, 6, 512, 27],\n",
              " [43, 6, 512, 27, 55],\n",
              " [43, 6, 512, 27, 55, 201],\n",
              " [43, 6, 512, 27, 55, 201, 440],\n",
              " [43, 6, 512, 27, 55, 201, 440, 1215]]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inp_sequences[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2FU2wGVVpZiy"
      },
      "outputs": [],
      "source": [
        "def generate_padded_sequences(input_sequences):\n",
        "  max_sequence_len = max([len(x) for x in input_sequences])\n",
        "  input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "  predictors, label = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "  label = ku.to_categorical(label, num_classes=total_words)\n",
        "  return predictors, label, max_sequence_len\n",
        "\n",
        "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XX4q5wKGqL0g",
        "outputId": "4cb50022-84c1-4abe-f29d-34b58cc8ca48"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "106"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_sequence_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80fJUm5TpZfO",
        "outputId": "a7c89df9-367b-4f70-aa33-aa1fbccae940"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 105, 10)           92210     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 100)               44400     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 100)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 9221)              931321    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1067931 (4.07 MB)\n",
            "Trainable params: 1067931 (4.07 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def create_model(max_sequence_len, total_words):\n",
        "  input_len = max_sequence_len - 1\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Embedding(total_words, 10, input_length=input_len))\n",
        "\n",
        "  model.add(LSTM(100))\n",
        "  model.add(Dropout(0.1))\n",
        "\n",
        "  model.add(Dense(total_words, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "  return model\n",
        "\n",
        "model = create_model(max_sequence_len, total_words)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SadWR_jKrEpl",
        "outputId": "eb75afc5-cf6d-42b1-c60d-cc1d58bec610"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "4326/4326 [==============================] - 73s 16ms/step - loss: 6.6462\n",
            "Epoch 2/100\n",
            "4326/4326 [==============================] - 42s 10ms/step - loss: 6.0628\n",
            "Epoch 3/100\n",
            "4326/4326 [==============================] - 43s 10ms/step - loss: 5.7037\n",
            "Epoch 4/100\n",
            "4326/4326 [==============================] - 43s 10ms/step - loss: 5.4700\n",
            "Epoch 5/100\n",
            "4326/4326 [==============================] - 44s 10ms/step - loss: 5.2964\n",
            "Epoch 6/100\n",
            "4326/4326 [==============================] - 45s 10ms/step - loss: 5.1499\n",
            "Epoch 7/100\n",
            "4326/4326 [==============================] - 46s 11ms/step - loss: 5.0238\n",
            "Epoch 8/100\n",
            "4326/4326 [==============================] - 45s 11ms/step - loss: 4.9077\n",
            "Epoch 9/100\n",
            "4326/4326 [==============================] - 44s 10ms/step - loss: 4.8019\n",
            "Epoch 10/100\n",
            "4326/4326 [==============================] - 39s 9ms/step - loss: 4.7032\n",
            "Epoch 11/100\n",
            "4326/4326 [==============================] - 39s 9ms/step - loss: 4.6139\n",
            "Epoch 12/100\n",
            "4326/4326 [==============================] - 39s 9ms/step - loss: 4.5278\n",
            "Epoch 13/100\n",
            "4326/4326 [==============================] - 40s 9ms/step - loss: 4.4508\n",
            "Epoch 14/100\n",
            "4326/4326 [==============================] - 41s 10ms/step - loss: 4.3799\n",
            "Epoch 15/100\n",
            "4326/4326 [==============================] - 41s 10ms/step - loss: 4.3118\n",
            "Epoch 16/100\n",
            "4326/4326 [==============================] - 41s 10ms/step - loss: 4.2490\n",
            "Epoch 17/100\n",
            "4326/4326 [==============================] - 39s 9ms/step - loss: 4.1924\n",
            "Epoch 18/100\n",
            "4326/4326 [==============================] - 39s 9ms/step - loss: 4.1357\n",
            "Epoch 19/100\n",
            "4326/4326 [==============================] - 39s 9ms/step - loss: 4.0843\n",
            "Epoch 20/100\n",
            "4326/4326 [==============================] - 39s 9ms/step - loss: 4.0347\n",
            "Epoch 21/100\n",
            "4326/4326 [==============================] - 38s 9ms/step - loss: 3.9926\n",
            "Epoch 22/100\n",
            "4326/4326 [==============================] - 45s 10ms/step - loss: 3.9477\n",
            "Epoch 23/100\n",
            "4326/4326 [==============================] - 44s 10ms/step - loss: 3.9088\n",
            "Epoch 24/100\n",
            "4326/4326 [==============================] - 45s 10ms/step - loss: 3.8719\n",
            "Epoch 25/100\n",
            "4326/4326 [==============================] - 45s 11ms/step - loss: 3.8383\n",
            "Epoch 26/100\n",
            "4326/4326 [==============================] - 45s 10ms/step - loss: 3.8074\n",
            "Epoch 27/100\n",
            "4326/4326 [==============================] - 39s 9ms/step - loss: 3.7804\n",
            "Epoch 28/100\n",
            "4326/4326 [==============================] - 39s 9ms/step - loss: 3.7507\n",
            "Epoch 29/100\n",
            "4326/4326 [==============================] - 39s 9ms/step - loss: 3.7238\n",
            "Epoch 30/100\n",
            "4326/4326 [==============================] - 40s 9ms/step - loss: 3.6952\n",
            "Epoch 31/100\n",
            "4326/4326 [==============================] - 41s 10ms/step - loss: 3.6745\n",
            "Epoch 32/100\n",
            "4326/4326 [==============================] - 41s 9ms/step - loss: 3.6542\n",
            "Epoch 33/100\n",
            "4326/4326 [==============================] - 41s 9ms/step - loss: 3.6364\n",
            "Epoch 34/100\n",
            "4326/4326 [==============================] - 41s 9ms/step - loss: 3.6169\n",
            "Epoch 35/100\n",
            "4326/4326 [==============================] - 40s 9ms/step - loss: 3.6007\n",
            "Epoch 36/100\n",
            "4326/4326 [==============================] - 44s 10ms/step - loss: 3.5823\n",
            "Epoch 37/100\n",
            "4326/4326 [==============================] - 43s 10ms/step - loss: 3.5680\n",
            "Epoch 38/100\n",
            "4326/4326 [==============================] - 44s 10ms/step - loss: 3.5562\n",
            "Epoch 39/100\n",
            "4326/4326 [==============================] - 45s 10ms/step - loss: 3.5367\n",
            "Epoch 40/100\n",
            "4326/4326 [==============================] - 44s 10ms/step - loss: 3.5230\n",
            "Epoch 41/100\n",
            "4326/4326 [==============================] - 44s 10ms/step - loss: 3.5172\n",
            "Epoch 42/100\n",
            "4326/4326 [==============================] - 44s 10ms/step - loss: 3.4986\n",
            "Epoch 43/100\n",
            "4326/4326 [==============================] - 45s 10ms/step - loss: 3.4896\n",
            "Epoch 44/100\n",
            "4326/4326 [==============================] - 44s 10ms/step - loss: 3.4825\n",
            "Epoch 45/100\n",
            "4326/4326 [==============================] - 44s 10ms/step - loss: 3.4738\n",
            "Epoch 46/100\n",
            "4326/4326 [==============================] - 43s 10ms/step - loss: 3.4621\n",
            "Epoch 47/100\n",
            "4326/4326 [==============================] - 44s 10ms/step - loss: 3.4515\n",
            "Epoch 48/100\n",
            "4326/4326 [==============================] - 44s 10ms/step - loss: 3.4424\n",
            "Epoch 49/100\n",
            "4326/4326 [==============================] - 44s 10ms/step - loss: 3.4377\n",
            "Epoch 50/100\n",
            "4326/4326 [==============================] - 41s 9ms/step - loss: 3.4295\n",
            "Epoch 51/100\n",
            "4326/4326 [==============================] - 39s 9ms/step - loss: 3.4206\n",
            "Epoch 52/100\n",
            "4326/4326 [==============================] - 39s 9ms/step - loss: 3.4131\n",
            "Epoch 53/100\n",
            "4326/4326 [==============================] - 39s 9ms/step - loss: 3.4102\n",
            "Epoch 54/100\n",
            "4326/4326 [==============================] - 39s 9ms/step - loss: 3.3985\n",
            "Epoch 55/100\n",
            "4326/4326 [==============================] - 39s 9ms/step - loss: 3.3936\n",
            "Epoch 56/100\n",
            "4326/4326 [==============================] - 38s 9ms/step - loss: 3.3916\n",
            "Epoch 57/100\n",
            "4326/4326 [==============================] - 39s 9ms/step - loss: 3.3849\n",
            "Epoch 58/100\n",
            "4326/4326 [==============================] - 39s 9ms/step - loss: 3.3800\n",
            "Epoch 59/100\n",
            "4326/4326 [==============================] - 39s 9ms/step - loss: 3.3708\n",
            "Epoch 60/100\n",
            "4326/4326 [==============================] - 39s 9ms/step - loss: 3.3727\n",
            "Epoch 61/100\n",
            "4326/4326 [==============================] - 39s 9ms/step - loss: 3.3634\n",
            "Epoch 62/100\n",
            "4326/4326 [==============================] - 39s 9ms/step - loss: 3.3628\n",
            "Epoch 63/100\n",
            "4326/4326 [==============================] - 39s 9ms/step - loss: 3.3558\n",
            "Epoch 64/100\n",
            "4326/4326 [==============================] - 39s 9ms/step - loss: 3.3492\n",
            "Epoch 65/100\n",
            "4326/4326 [==============================] - 39s 9ms/step - loss: 3.3467\n",
            "Epoch 66/100\n",
            "4326/4326 [==============================] - 38s 9ms/step - loss: 3.3491\n",
            "Epoch 67/100\n",
            "4326/4326 [==============================] - 39s 9ms/step - loss: 3.3348\n",
            "Epoch 68/100\n",
            "4326/4326 [==============================] - 39s 9ms/step - loss: 3.3280\n",
            "Epoch 69/100\n",
            "4326/4326 [==============================] - 39s 9ms/step - loss: 3.3313\n",
            "Epoch 70/100\n",
            "4326/4326 [==============================] - 41s 9ms/step - loss: 3.3309\n",
            "Epoch 71/100\n",
            "4326/4326 [==============================] - 40s 9ms/step - loss: 3.3260\n",
            "Epoch 72/100\n",
            "4326/4326 [==============================] - 40s 9ms/step - loss: 3.3233\n",
            "Epoch 73/100\n",
            "4326/4326 [==============================] - 41s 10ms/step - loss: 3.3167\n",
            "Epoch 74/100\n",
            "4326/4326 [==============================] - 41s 9ms/step - loss: 3.3176\n",
            "Epoch 75/100\n",
            "4326/4326 [==============================] - 41s 10ms/step - loss: 3.3145\n",
            "Epoch 76/100\n",
            "4326/4326 [==============================] - 41s 9ms/step - loss: 3.3183\n",
            "Epoch 77/100\n",
            "4326/4326 [==============================] - 45s 10ms/step - loss: 3.3078\n",
            "Epoch 78/100\n",
            "4326/4326 [==============================] - 45s 10ms/step - loss: 3.3059\n",
            "Epoch 79/100\n",
            "4326/4326 [==============================] - 45s 10ms/step - loss: 3.3084\n",
            "Epoch 80/100\n",
            "4326/4326 [==============================] - 44s 10ms/step - loss: 3.3057\n",
            "Epoch 81/100\n",
            "4326/4326 [==============================] - 44s 10ms/step - loss: 3.3029\n",
            "Epoch 82/100\n",
            "4326/4326 [==============================] - 39s 9ms/step - loss: 3.2985\n",
            "Epoch 83/100\n",
            "4326/4326 [==============================] - 39s 9ms/step - loss: 3.2951\n",
            "Epoch 84/100\n",
            "4326/4326 [==============================] - 39s 9ms/step - loss: 3.2992\n",
            "Epoch 85/100\n",
            "4326/4326 [==============================] - 40s 9ms/step - loss: 3.2903\n",
            "Epoch 86/100\n",
            "4326/4326 [==============================] - 40s 9ms/step - loss: 3.2885\n",
            "Epoch 87/100\n",
            "4326/4326 [==============================] - 42s 10ms/step - loss: 3.2881\n",
            "Epoch 88/100\n",
            "4326/4326 [==============================] - 41s 9ms/step - loss: 3.2818\n",
            "Epoch 89/100\n",
            "4326/4326 [==============================] - 41s 9ms/step - loss: 3.2822\n",
            "Epoch 90/100\n",
            "4326/4326 [==============================] - 42s 10ms/step - loss: 3.2848\n",
            "Epoch 91/100\n",
            "4326/4326 [==============================] - 45s 10ms/step - loss: 3.2843\n",
            "Epoch 92/100\n",
            "4326/4326 [==============================] - 44s 10ms/step - loss: 3.2811\n",
            "Epoch 93/100\n",
            "4326/4326 [==============================] - 44s 10ms/step - loss: 3.2783\n",
            "Epoch 94/100\n",
            "4326/4326 [==============================] - 44s 10ms/step - loss: 3.2839\n",
            "Epoch 95/100\n",
            "4326/4326 [==============================] - 43s 10ms/step - loss: 3.2826\n",
            "Epoch 96/100\n",
            "4326/4326 [==============================] - 39s 9ms/step - loss: 3.2810\n",
            "Epoch 97/100\n",
            "4326/4326 [==============================] - 39s 9ms/step - loss: 3.2725\n",
            "Epoch 98/100\n",
            "4326/4326 [==============================] - 39s 9ms/step - loss: 3.2787\n",
            "Epoch 99/100\n",
            "4326/4326 [==============================] - 40s 9ms/step - loss: 3.2727\n",
            "Epoch 100/100\n",
            "4326/4326 [==============================] - 41s 9ms/step - loss: 3.2784\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7bd3acaabc70>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(predictors, label, epochs = 100, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rd86fTgGuSaj",
        "outputId": "443d44fd-4a87-41f7-f20a-6e5ea569f000"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<keras.src.engine.sequential.Sequential at 0x7bd3b19cf370>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "S6B4fBqFrOeo"
      },
      "outputs": [],
      "source": [
        "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
        "  for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted = model.predict([token_list], verbose=0)\n",
        "    predicted = np.argmax(predicted, axis=1)\n",
        "    output_words = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "      if index == predicted:\n",
        "        output_word = word\n",
        "        break\n",
        "    seed_text += \" \" + output_word\n",
        "  return seed_text.title()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wL2TF4tOthJu",
        "outputId": "f8847c1a-7fc5-4bdc-d57c-3ae06628332b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kings Landing Was The Broken Tower Of The Mountain And The Great Brothers Had A Cavernous Blue Of Least Its Bronze Scales Of Black Age And A Heirs Of Twelve Bread And Menatarms And Blood And Rotting Black Leaves Black Iron Decent Body Armored With Iron Iron Wives And Hundreds Of Wagons\n"
          ]
        }
      ],
      "source": [
        "# print(generate_text(\"Tyrion\", 200, model, max_sequence_len))\n",
        "print(generate_text(\"Kings landing\", 500, model, max_sequence_len))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
